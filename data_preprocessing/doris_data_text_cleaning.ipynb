{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfFileReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(pdf_reader.getNumPages()):\n",
    "            page = pdf_reader.getPage(page_num)\n",
    "            text += page.extractText()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "ocr_not_required_dir = \"../data/OCR_Not_Required\"\n",
    "pdf_files = [file for file in os.listdir(ocr_not_required_dir) if file.endswith(\".pdf\")]\n",
    "\n",
    "pdf_texts = []\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(ocr_not_required_dir, pdf_file)\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    pdf_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_performed_dir = \"../data/OCR_Performed\"\n",
    "text_files = [file for file in os.listdir(ocr_performed_dir) if file.endswith(\".txt\")]\n",
    "\n",
    "ocr_texts = []\n",
    "for text_file in text_files:\n",
    "    text_path = os.path.join(ocr_performed_dir, text_file)\n",
    "    with open(text_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "        ocr_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents loaded: 19\n"
     ]
    }
   ],
   "source": [
    "all_texts = []\n",
    "all_texts.extend(pdf_texts)\n",
    "all_texts.extend(ocr_texts)\n",
    "print(f\"Total number of documents loaded: {len(all_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespaces and line breaks\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = [clean_text(text) for text in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 automated speed enforcement program report 20142016 2 in 2013 the state legislature and governor cuomo enacted sec 1180b of new york states vehicle and traffic law vtl which granted new york city the authority to pilot an automated speed enforcement program to deter speeding in 20 school zones the first speed camera violation was issued in january 2014 in june 2014 the pilot was expanded to 140 school zones in or der to support the pursuit of the citys vision zero goal to eliminate traffic dea\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_texts[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenization, stopword removal, and lemmatization\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and not token.is_punct\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_texts = [preprocess_text(text) for text in cleaned_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams for the first document: ['1 automated', 'automated speed', 'speed enforcement', 'enforcement program', 'program report', 'report 20142016', '20142016 2', '2 in', 'in 2013', '2013 the']\n",
      "Trigrams for the first document: ['1 automated speed', 'automated speed enforcement', 'speed enforcement program', 'enforcement program report', 'program report 20142016', 'report 20142016 2', '20142016 2 in', '2 in 2013', 'in 2013 the', '2013 the state']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to extract bigrams\n",
    "def extract_bigrams(text):\n",
    "    bigrams = list(ngrams(text.split(), 2))\n",
    "    bigram_phrases = [' '.join(bigram) for bigram in bigrams]\n",
    "    return bigram_phrases\n",
    "\n",
    "# Define a function to extract trigrams\n",
    "def extract_trigrams(text):\n",
    "    trigrams = list(ngrams(text.split(), 3))\n",
    "    trigram_phrases = [' '.join(trigram) for trigram in trigrams]\n",
    "    return trigram_phrases\n",
    "\n",
    "# Extract bigrams and trigrams for each cleaned text\n",
    "bigrams_list = []\n",
    "trigrams_list = []\n",
    "\n",
    "for cleaned_text in cleaned_texts:\n",
    "    bigrams = extract_bigrams(cleaned_text)\n",
    "    trigrams = extract_trigrams(cleaned_text)\n",
    "    bigrams_list.append(bigrams)\n",
    "    trigrams_list.append(trigrams)\n",
    "\n",
    "# Check the first few bigrams and trigrams for the first document\n",
    "print(\"Bigrams for the first document:\", bigrams_list[0][:10])\n",
    "print(\"Trigrams for the first document:\", trigrams_list[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Cleaned_Text/doris_cleaned_texts.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for text in cleaned_texts:\n",
    "        writer.writerow([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(\"../data/Cleaned_Text\", exist_ok=True)\n",
    "\n",
    "with open(\"../data/Cleaned_Text/doris_cleaned_texts.json\", \"w\") as outfile:\n",
    "    json.dump(cleaned_texts, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
